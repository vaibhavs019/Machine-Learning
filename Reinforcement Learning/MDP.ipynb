{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "**Experiment-5**"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1579fd2f710cf0f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Aim: Implementation of MDP for a specific application**"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c9013fd960f253c9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Theory:**\n",
    "A Markov decision process (MDP) refers to a stochastic decision-making process that uses a mathematical framework to model the decision-making of a dynamic system. It is used in scenarios where the results are either random or controlled by a decision maker, which makes sequential decisions over time. MDPs evaluate which actions the decision maker should take considering the current state and environment of the system.\n",
    "\n",
    "MDPs rely on variables such as the environment, agent’s actions, and rewards to decide the system’s next optimal action. They are classified into four types — finite, infinite, continuous, or discrete — depending on various factors such as sets of actions, available states, and the decision-making frequency.\n",
    "\n",
    "MDP models are typically popular in two sub-areas of AI: probabilistic planning and reinforcement learning (RL). \n",
    "\n",
    "1. Probabilistic planning is the discipline that uses known models to accomplish an agent’s goals and objectives. While doing so, it emphasizes guiding machines or agents to make decisions while enabling them to learn how to behave to achieve their goals. \n",
    "2. Reinforcement learning allows applications to learn from the feedback the agents receive from the environment."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2cb70c370f91de5d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Working:**\n",
    "The MDP model operates by using key elements such as the agent, states, actions, rewards, and optimal policies. The agent refers to a system responsible for making decisions and performing actions. It operates in an environment that details the various states that the agent is in while it transitions from one state to another. MDP defines the mechanism of how certain states and an agent’s actions lead to the other states. Moreover, the agent receives rewards depending on the action it performs and the state it attains (current state). The policy for the MDP model reveals the agent’s following action depending on its current state.\n",
    "\n",
    "The MDP framework has the following key components:\n",
    "\n",
    "S: states (s ∈ S)\n",
    "A: Actions (a ∈ A)\n",
    "P (St+1|st.at): Transition probabilities\n",
    "R (s): Reward\n",
    "\n",
    "The graphical representation of the MDP model is as follows:\n",
    "![](/Users/skull/Downloads/MDP-model.png?raw=true)\n",
    "\n",
    "\n",
    "The MDP model uses the Markov Property, which states that the future can be determined only from the present state that encapsulates all the necessary information from the past. The Markov Property can be evaluated by using this equation:\n",
    "\n",
    "P[St+1|St] = P[St+1 |S1,S2,S3……St]\n",
    "\n",
    "According to this equation, the probability of the next state (P[St+1]) given the present state (St) is given by the next state’s probability (P[St+1]) considering all the previous states (S1,S2,S3……St). This implies that MDP uses only the present/current state to evaluate the next actions without any dependencies on previous states or actions."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f60abab68dabf63f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Examples:**\n",
    "1. Routing problems.\n",
    "2. Managing maintenance and repair of dynamic system.\n",
    "3. Designing intelligent machines.\n",
    "4. Designing quiz games.\n",
    "5. Managing wait time at a traffic intersection.\n",
    "6. Determining the number of patients to admit to a hospital."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e5706a72dc33813f"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-04-05T14:16:46.886967Z",
     "start_time": "2024-04-05T14:16:46.664592Z"
    }
   },
   "outputs": [],
   "source": [
    "from typing import Tuple, Dict, Optional, Iterable\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "from matplotlib import animation\n",
    "from IPython.display import HTML\n",
    "import gym\n",
    "from gym import spaces\n",
    "from gym.error import DependencyNotInstalled\n",
    "import pygame\n",
    "from pygame import gfxdraw\n",
    "\n",
    "\n",
    "class Maze(gym.Env):\n",
    "\n",
    "    def __init__(self, exploring_starts: bool = False,\n",
    "                 shaped_rewards: bool = False, size: int = 5) -> None:\n",
    "        super().__init__()\n",
    "        self.exploring_starts = exploring_starts\n",
    "        self.shaped_rewards = shaped_rewards\n",
    "        self.state = (size - 1, size - 1)\n",
    "        self.goal = (size - 1, size - 1)\n",
    "        self.maze = self._create_maze(size=size)\n",
    "        self.distances = self._compute_distances(self.goal, self.maze)\n",
    "        self.action_space = spaces.Discrete(n=4)\n",
    "        self.action_space.action_meanings = {0: 'UP', 1: 'RIGHT', 2: 'DOWN', 3: \"LEFT\"}\n",
    "        self.observation_space = spaces.MultiDiscrete([size, size])\n",
    "\n",
    "        self.screen = None\n",
    "        self.agent_transform = None\n",
    "\n",
    "    def step(self, action: int) -> Tuple[Tuple[int, int], float, bool, Dict]:\n",
    "        reward = self.compute_reward(self.state, action)\n",
    "        self.state = self._get_next_state(self.state, action)\n",
    "        done = self.state == self.goal\n",
    "        info = {}\n",
    "        return self.state, reward, done, info\n",
    "\n",
    "    def reset(self) -> Tuple[int, int]:\n",
    "        if self.exploring_starts:\n",
    "            while self.state == self.goal:\n",
    "                self.state = tuple(self.observation_space.sample())\n",
    "        else:\n",
    "            self.state = (0, 0)\n",
    "        return self.state\n",
    "\n",
    "    def render(self, mode: str = 'human') -> Optional[np.ndarray]:\n",
    "        assert mode in ['human', 'rgb_array']\n",
    "\n",
    "        screen_size = 600\n",
    "        scale = screen_size / 5\n",
    "\n",
    "        if self.screen is None:\n",
    "            pygame.init()\n",
    "            self.screen = pygame.Surface((screen_size, screen_size))\n",
    "\n",
    "        surf = pygame.Surface((screen_size, screen_size))\n",
    "        surf.fill((22, 36, 71))\n",
    "\n",
    "\n",
    "        for row in range(5):\n",
    "            for col in range(5):\n",
    "\n",
    "                state = (row, col)\n",
    "                for next_state in [(row + 1, col), (row - 1, col), (row, col + 1), (row, col - 1)]:\n",
    "                    if next_state not in self.maze[state]:\n",
    "\n",
    "                        # Add the geometry of the edges and walls (i.e. the boundaries between\n",
    "                        # adjacent squares that are not connected).\n",
    "                        row_diff, col_diff = np.subtract(next_state, state)\n",
    "                        left = (col + (col_diff > 0)) * scale - 2 * (col_diff != 0)\n",
    "                        right = ((col + 1) - (col_diff < 0)) * scale + 2 * (col_diff != 0)\n",
    "                        top = (5 - (row + (row_diff > 0))) * scale - 2 * (row_diff != 0)\n",
    "                        bottom = (5 - ((row + 1) - (row_diff < 0))) * scale + 2 * (row_diff != 0)\n",
    "\n",
    "                        gfxdraw.filled_polygon(surf, [(left, bottom), (left, top), (right, top), (right, bottom)], (255, 255, 255))\n",
    "\n",
    "        # Add the geometry of the goal square to the viewer.\n",
    "        left, right, top, bottom = scale * 4 + 10, scale * 5 - 10, scale - 10, 10\n",
    "        gfxdraw.filled_polygon(surf, [(left, bottom), (left, top), (right, top), (right, bottom)], (40, 199, 172))\n",
    "\n",
    "        # Add the geometry of the agent to the viewer.\n",
    "        agent_row = int(screen_size - scale * (self.state[0] + .5))\n",
    "        agent_col = int(scale * (self.state[1] + .5))\n",
    "        gfxdraw.filled_circle(surf, agent_col, agent_row, int(scale * .6 / 2), (228, 63, 90))\n",
    "\n",
    "        surf = pygame.transform.flip(surf, False, True)\n",
    "        self.screen.blit(surf, (0, 0))\n",
    "\n",
    "        return np.transpose(\n",
    "            np.array(pygame.surfarray.pixels3d(self.screen)), axes=(1, 0, 2)\n",
    "        )\n",
    "\n",
    "    def close(self) -> None:\n",
    "        if self.screen is not None:\n",
    "            pygame.display.quit()\n",
    "            pygame.quit()\n",
    "            self.screen = None\n",
    "\n",
    "    def compute_reward(self, state: Tuple[int, int], action: int) -> float:\n",
    "        next_state = self._get_next_state(state, action)\n",
    "        if self.shaped_rewards:\n",
    "            return - (self.distances[next_state] / self.distances.max())\n",
    "        return - float(state != self.goal)\n",
    "\n",
    "    def simulate_step(self, state: Tuple[int, int], action: int):\n",
    "        reward = self.compute_reward(state, action)\n",
    "        next_state = self._get_next_state(state, action)\n",
    "        done = next_state == self.goal\n",
    "        info = {}\n",
    "        return next_state, reward, done, info\n",
    "\n",
    "    def _get_next_state(self, state: Tuple[int, int], action: int) -> Tuple[int, int]:\n",
    "        if action == 0:\n",
    "            next_state = (state[0] - 1, state[1])\n",
    "        elif action == 1:\n",
    "            next_state = (state[0], state[1] + 1)\n",
    "        elif action == 2:\n",
    "            next_state = (state[0] + 1, state[1])\n",
    "        elif action == 3:\n",
    "            next_state = (state[0], state[1] - 1)\n",
    "        else:\n",
    "            raise ValueError(\"Action value not supported:\", action)\n",
    "        if next_state in self.maze[state]:\n",
    "            return next_state\n",
    "        return state\n",
    "\n",
    "    @staticmethod\n",
    "    def _create_maze(size: int) -> Dict[Tuple[int, int], Iterable[Tuple[int, int]]]:\n",
    "        maze = {(row, col): [(row - 1, col), (row + 1, col), (row, col - 1), (row, col + 1)]\n",
    "                for row in range(size) for col in range(size)}\n",
    "\n",
    "        left_edges = [[(row, 0), (row, -1)] for row in range(size)]\n",
    "        right_edges = [[(row, size - 1), (row, size)] for row in range(size)]\n",
    "        upper_edges = [[(0, col), (-1, col)] for col in range(size)]\n",
    "        lower_edges = [[(size - 1, col), (size, col)] for col in range(size)]\n",
    "        walls = [\n",
    "            [(1, 0), (1, 1)], [(2, 0), (2, 1)], [(3, 0), (3, 1)],\n",
    "            [(1, 1), (1, 2)], [(2, 1), (2, 2)], [(3, 1), (3, 2)],\n",
    "            [(3, 1), (4, 1)], [(0, 2), (1, 2)], [(1, 2), (1, 3)],\n",
    "            [(2, 2), (3, 2)], [(2, 3), (3, 3)], [(2, 4), (3, 4)],\n",
    "            [(4, 2), (4, 3)], [(1, 3), (1, 4)], [(2, 3), (2, 4)],\n",
    "        ]\n",
    "\n",
    "        obstacles = upper_edges + lower_edges + left_edges + right_edges + walls\n",
    "\n",
    "        for src, dst in obstacles:\n",
    "            maze[src].remove(dst)\n",
    "\n",
    "            if dst in maze:\n",
    "                maze[dst].remove(src)\n",
    "\n",
    "        return maze\n",
    "\n",
    "    @staticmethod\n",
    "    def _compute_distances(goal: Tuple[int, int],\n",
    "                           maze: Dict[Tuple[int, int], Iterable[Tuple[int, int]]]) -> np.ndarray:\n",
    "        distances = np.full((5, 5), np.inf)\n",
    "        visited = set()\n",
    "        distances[goal] = 0.\n",
    "\n",
    "        while visited != set(maze):\n",
    "            sorted_dst = [(v // 5, v % 5) for v in distances.argsort(axis=None)]\n",
    "            closest = next(x for x in sorted_dst if x not in visited)\n",
    "            visited.add(closest)\n",
    "\n",
    "            for neighbour in maze[closest]:\n",
    "                distances[neighbour] = min(distances[neighbour], distances[closest] + 1)\n",
    "        return distances\n",
    "\n",
    "\n",
    "def display_video(frames):\n",
    "    # Copied from: https://colab.research.google.com/github/deepmind/dm_control/blob/master/tutorial.ipynb\n",
    "    orig_backend = matplotlib.get_backend()\n",
    "    matplotlib.use('Agg')\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(5, 5))\n",
    "    matplotlib.use(orig_backend)\n",
    "    ax.set_axis_off()\n",
    "    ax.set_aspect('equal')\n",
    "    ax.set_position([0, 0, 1, 1])\n",
    "    im = ax.imshow(frames[0])\n",
    "    def update(frame):\n",
    "        im.set_data(frame)\n",
    "        return [im]\n",
    "    anim = animation.FuncAnimation(fig=fig, func=update, frames=frames,\n",
    "                                   interval=50, blit=True, repeat=False)\n",
    "    return HTML(anim.to_html5_video())"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import gym \n",
    "import numpy as np\n",
    "from IPython import display\n",
    "from matplotlib import pyplot as plt"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-05T14:16:46.889267Z",
     "start_time": "2024-04-05T14:16:46.887609Z"
    }
   },
   "id": "d4f219593888dcb7",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "env = Maze()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-05T14:16:46.892926Z",
     "start_time": "2024-04-05T14:16:46.891021Z"
    }
   },
   "id": "691c01c24a442bec",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The new episode will start in state: (0, 0)\n"
     ]
    }
   ],
   "source": [
    "initial_state = env.reset()\n",
    "print(f\"The new episode will start in state: {initial_state}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-05T14:16:46.895632Z",
     "start_time": "2024-04-05T14:16:46.893645Z"
    }
   },
   "id": "d88ea0b26c39a6e9",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "<matplotlib.image.AxesImage at 0x13f97a680>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "frame = env.render(mode='rgb_array')\n",
    "plt.axis('off')\n",
    "plt.title(f\"State: {initial_state}\")\n",
    "plt.imshow(frame)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-05T14:16:47.358347Z",
     "start_time": "2024-04-05T14:16:46.896274Z"
    }
   },
   "id": "8beeff9f3240989e",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After moving down 1 row, the agent is in state: (1, 0)\n",
      "After moving down 1 row, we got a reward of: -1.0\n",
      "After moving down 1 row, the task is not finished\n"
     ]
    }
   ],
   "source": [
    "action = 2\n",
    "next_state, reward, done, info = env.step(action)\n",
    "print(f\"After moving down 1 row, the agent is in state: {next_state}\")\n",
    "print(f\"After moving down 1 row, we got a reward of: {reward}\")\n",
    "print(\"After moving down 1 row, the task is\", \"\" if done else \"not\", \"finished\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-05T14:16:47.361366Z",
     "start_time": "2024-04-05T14:16:47.359244Z"
    }
   },
   "id": "b734f767f77cb21f",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "<matplotlib.image.AxesImage at 0x167eb2d70>"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "frame = env.render(mode='rgb_array')\n",
    "plt.axis('off')\n",
    "plt.title(f\"State: {next_state}\")\n",
    "plt.imshow(frame)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-05T14:16:47.461924Z",
     "start_time": "2024-04-05T14:16:47.362156Z"
    }
   },
   "id": "763450197f0f0755",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "env.close()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-05T14:16:47.554828Z",
     "start_time": "2024-04-05T14:16:47.463024Z"
    }
   },
   "id": "f7e806f66423f119",
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "env = Maze()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-05T14:16:47.557595Z",
     "start_time": "2024-04-05T14:16:47.555541Z"
    }
   },
   "id": "32c9476d48ec24cd",
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "(0, 0)"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-05T14:16:47.560461Z",
     "start_time": "2024-04-05T14:16:47.558266Z"
    }
   },
   "id": "7ad355f6f7947e32",
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "MultiDiscrete([5 5])"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-05T14:16:47.563408Z",
     "start_time": "2024-04-05T14:16:47.561152Z"
    }
   },
   "id": "a7674655ea6174c1",
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "Discrete(4)"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-05T14:16:47.567981Z",
     "start_time": "2024-04-05T14:16:47.565811Z"
    }
   },
   "id": "f94a396a5c208f1e",
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "0"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.sample()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-05T14:16:47.571126Z",
     "start_time": "2024-04-05T14:16:47.568521Z"
    }
   },
   "id": "1564456280b70945",
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "source": [
    "Trajectory creation for 3 actions"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a92ecfb3e09083b"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "env = Maze()\n",
    "state = env.reset()\n",
    "trajectory = []\n",
    "for _ in range(3):\n",
    "    action = env.action_space.sample()\n",
    "    next_state, reward, done, extra_info = env.step(action)\n",
    "    trajectory.append([state, action, reward, done, next_state])\n",
    "    state = next_state\n",
    "env.close()    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-05T14:16:47.574675Z",
     "start_time": "2024-04-05T14:16:47.572013Z"
    }
   },
   "id": "ab995d23f0dbdf14",
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First trajectoy is: \n",
      "[[(0, 0), 1, -1.0, False, (0, 1)], [(0, 1), 0, -1.0, False, (0, 1)], [(0, 1), 3, -1.0, False, (0, 0)]]\n"
     ]
    }
   ],
   "source": [
    "print(f\"First trajectoy is: \\n{trajectory}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-05T14:16:47.576911Z",
     "start_time": "2024-04-05T14:16:47.575293Z"
    }
   },
   "id": "e951868bbe9a0d6c",
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "env = Maze()\n",
    "state = env.reset()\n",
    "episode = []\n",
    "done = False\n",
    "while not done:\n",
    "    action = env.action_space.sample()\n",
    "    next_state, reward, done, extra_info = env.step(action)\n",
    "    episode.append([state, action, reward, done, next_state])\n",
    "    state = next_state\n",
    "env.close()    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-05T14:16:47.580453Z",
     "start_time": "2024-04-05T14:16:47.577669Z"
    }
   },
   "id": "f662bb6d63fd0984",
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "[[(0, 0), 3, -1.0, False, (0, 0)],\n [(0, 0), 2, -1.0, False, (1, 0)],\n [(1, 0), 3, -1.0, False, (1, 0)],\n [(1, 0), 1, -1.0, False, (1, 0)],\n [(1, 0), 0, -1.0, False, (0, 0)],\n [(0, 0), 2, -1.0, False, (1, 0)],\n [(1, 0), 2, -1.0, False, (2, 0)],\n [(2, 0), 1, -1.0, False, (2, 0)],\n [(2, 0), 3, -1.0, False, (2, 0)],\n [(2, 0), 3, -1.0, False, (2, 0)],\n [(2, 0), 0, -1.0, False, (1, 0)],\n [(1, 0), 3, -1.0, False, (1, 0)],\n [(1, 0), 2, -1.0, False, (2, 0)],\n [(2, 0), 3, -1.0, False, (2, 0)],\n [(2, 0), 1, -1.0, False, (2, 0)],\n [(2, 0), 1, -1.0, False, (2, 0)],\n [(2, 0), 2, -1.0, False, (3, 0)],\n [(3, 0), 1, -1.0, False, (3, 0)],\n [(3, 0), 3, -1.0, False, (3, 0)],\n [(3, 0), 0, -1.0, False, (2, 0)],\n [(2, 0), 3, -1.0, False, (2, 0)],\n [(2, 0), 0, -1.0, False, (1, 0)],\n [(1, 0), 1, -1.0, False, (1, 0)],\n [(1, 0), 2, -1.0, False, (2, 0)],\n [(2, 0), 1, -1.0, False, (2, 0)],\n [(2, 0), 0, -1.0, False, (1, 0)],\n [(1, 0), 2, -1.0, False, (2, 0)],\n [(2, 0), 1, -1.0, False, (2, 0)],\n [(2, 0), 3, -1.0, False, (2, 0)],\n [(2, 0), 2, -1.0, False, (3, 0)],\n [(3, 0), 2, -1.0, False, (4, 0)],\n [(4, 0), 1, -1.0, False, (4, 1)],\n [(4, 1), 0, -1.0, False, (4, 1)],\n [(4, 1), 2, -1.0, False, (4, 1)],\n [(4, 1), 2, -1.0, False, (4, 1)],\n [(4, 1), 1, -1.0, False, (4, 2)],\n [(4, 2), 0, -1.0, False, (3, 2)],\n [(3, 2), 3, -1.0, False, (3, 2)],\n [(3, 2), 0, -1.0, False, (3, 2)],\n [(3, 2), 0, -1.0, False, (3, 2)],\n [(3, 2), 3, -1.0, False, (3, 2)],\n [(3, 2), 1, -1.0, False, (3, 3)],\n [(3, 3), 3, -1.0, False, (3, 2)],\n [(3, 2), 3, -1.0, False, (3, 2)],\n [(3, 2), 2, -1.0, False, (4, 2)],\n [(4, 2), 2, -1.0, False, (4, 2)],\n [(4, 2), 3, -1.0, False, (4, 1)],\n [(4, 1), 1, -1.0, False, (4, 2)],\n [(4, 2), 2, -1.0, False, (4, 2)],\n [(4, 2), 0, -1.0, False, (3, 2)],\n [(3, 2), 0, -1.0, False, (3, 2)],\n [(3, 2), 3, -1.0, False, (3, 2)],\n [(3, 2), 1, -1.0, False, (3, 3)],\n [(3, 3), 3, -1.0, False, (3, 2)],\n [(3, 2), 2, -1.0, False, (4, 2)],\n [(4, 2), 1, -1.0, False, (4, 2)],\n [(4, 2), 2, -1.0, False, (4, 2)],\n [(4, 2), 0, -1.0, False, (3, 2)],\n [(3, 2), 0, -1.0, False, (3, 2)],\n [(3, 2), 0, -1.0, False, (3, 2)],\n [(3, 2), 2, -1.0, False, (4, 2)],\n [(4, 2), 1, -1.0, False, (4, 2)],\n [(4, 2), 0, -1.0, False, (3, 2)],\n [(3, 2), 3, -1.0, False, (3, 2)],\n [(3, 2), 2, -1.0, False, (4, 2)],\n [(4, 2), 3, -1.0, False, (4, 1)],\n [(4, 1), 2, -1.0, False, (4, 1)],\n [(4, 1), 1, -1.0, False, (4, 2)],\n [(4, 2), 1, -1.0, False, (4, 2)],\n [(4, 2), 1, -1.0, False, (4, 2)],\n [(4, 2), 3, -1.0, False, (4, 1)],\n [(4, 1), 2, -1.0, False, (4, 1)],\n [(4, 1), 0, -1.0, False, (4, 1)],\n [(4, 1), 2, -1.0, False, (4, 1)],\n [(4, 1), 3, -1.0, False, (4, 0)],\n [(4, 0), 2, -1.0, False, (4, 0)],\n [(4, 0), 2, -1.0, False, (4, 0)],\n [(4, 0), 3, -1.0, False, (4, 0)],\n [(4, 0), 3, -1.0, False, (4, 0)],\n [(4, 0), 3, -1.0, False, (4, 0)],\n [(4, 0), 3, -1.0, False, (4, 0)],\n [(4, 0), 3, -1.0, False, (4, 0)],\n [(4, 0), 3, -1.0, False, (4, 0)],\n [(4, 0), 3, -1.0, False, (4, 0)],\n [(4, 0), 0, -1.0, False, (3, 0)],\n [(3, 0), 3, -1.0, False, (3, 0)],\n [(3, 0), 1, -1.0, False, (3, 0)],\n [(3, 0), 3, -1.0, False, (3, 0)],\n [(3, 0), 0, -1.0, False, (2, 0)],\n [(2, 0), 3, -1.0, False, (2, 0)],\n [(2, 0), 1, -1.0, False, (2, 0)],\n [(2, 0), 1, -1.0, False, (2, 0)],\n [(2, 0), 3, -1.0, False, (2, 0)],\n [(2, 0), 2, -1.0, False, (3, 0)],\n [(3, 0), 0, -1.0, False, (2, 0)],\n [(2, 0), 2, -1.0, False, (3, 0)],\n [(3, 0), 2, -1.0, False, (4, 0)],\n [(4, 0), 3, -1.0, False, (4, 0)],\n [(4, 0), 3, -1.0, False, (4, 0)],\n [(4, 0), 2, -1.0, False, (4, 0)],\n [(4, 0), 0, -1.0, False, (3, 0)],\n [(3, 0), 1, -1.0, False, (3, 0)],\n [(3, 0), 3, -1.0, False, (3, 0)],\n [(3, 0), 2, -1.0, False, (4, 0)],\n [(4, 0), 1, -1.0, False, (4, 1)],\n [(4, 1), 2, -1.0, False, (4, 1)],\n [(4, 1), 1, -1.0, False, (4, 2)],\n [(4, 2), 1, -1.0, False, (4, 2)],\n [(4, 2), 0, -1.0, False, (3, 2)],\n [(3, 2), 1, -1.0, False, (3, 3)],\n [(3, 3), 3, -1.0, False, (3, 2)],\n [(3, 2), 2, -1.0, False, (4, 2)],\n [(4, 2), 0, -1.0, False, (3, 2)],\n [(3, 2), 0, -1.0, False, (3, 2)],\n [(3, 2), 1, -1.0, False, (3, 3)],\n [(3, 3), 0, -1.0, False, (3, 3)],\n [(3, 3), 1, -1.0, False, (3, 4)],\n [(3, 4), 1, -1.0, False, (3, 4)],\n [(3, 4), 0, -1.0, False, (3, 4)],\n [(3, 4), 1, -1.0, False, (3, 4)],\n [(3, 4), 2, -1.0, True, (4, 4)]]"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "episode"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-05T14:16:47.590253Z",
     "start_time": "2024-04-05T14:16:47.581180Z"
    }
   },
   "id": "50eaf5323446ea4b",
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "env = Maze()\n",
    "state = env.reset()\n",
    "done = False\n",
    "gamma = 0.98\n",
    "G_0 = 0\n",
    "t = 0\n",
    "while not done:\n",
    "    action = env.action_space.sample()\n",
    "    next_state, reward, done, extra_info = env.step(action)\n",
    "    G_0 += gamma ** t * reward\n",
    "    t += 1\n",
    "env.close()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-05T14:16:47.593968Z",
     "start_time": "2024-04-05T14:16:47.590902Z"
    }
   },
   "id": "43aaa8518984b748",
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "76"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#No of iterations\n",
    "t"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-05T14:16:47.597301Z",
     "start_time": "2024-04-05T14:16:47.594554Z"
    }
   },
   "id": "e3b4f083e614f655",
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "-39.231585678215446"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Total reward\n",
    "G_0"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-05T14:16:47.600562Z",
     "start_time": "2024-04-05T14:16:47.598126Z"
    }
   },
   "id": "f134c5477fdeb6c5",
   "execution_count": 20
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Conclusion**\n",
    "The Markov decision process is a stochastic decision-making tool based on the Markov Property principle. It is used to make optimal decisions for dynamic systems while considering their current state and the environment in which they operate. MDP is a key component of reinforcement learning applications and is widely employed to design intelligent systems. Several industries, such as robotic process automation, manufacturing, finance & economics, and logistics, use MDPs regularly to carry out their day-to-day tasks."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "eb237227e8342725"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-05T14:16:47.602633Z",
     "start_time": "2024-04-05T14:16:47.601285Z"
    }
   },
   "id": "4541b6b0c7e368d7",
   "execution_count": 20
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
